{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPTZC3Bsv0LG"
   },
   "source": [
    "# Librerías\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1751485772446,
     "user": {
      "displayName": "Gastón Rimbano",
      "userId": "17074264266222486732"
     },
     "user_tz": -120
    },
    "id": "MlTiSQvkv0jZ"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sqlglot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5pAKrj5IIPz"
   },
   "source": [
    "# Variables globales\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1751485772454,
     "user": {
      "displayName": "Gastón Rimbano",
      "userId": "17074264266222486732"
     },
     "user_tz": -120
    },
    "id": "stToljf2_p99"
   },
   "outputs": [],
   "source": [
    "READ_DATA_PATH = '../data/database/tsql'\n",
    "FULL_SCRIPT_PATH = f'{READ_DATA_PATH}/instawdbdw.sql'\n",
    "SQL_ENCODING = 'UTF-16'\n",
    "\n",
    "SAMPLES_ABS_PATH = ''\n",
    "SAMPLES_ENCODING = 'UTF16'\n",
    "SAMPLES_DELIMITER = '|'\n",
    "\n",
    "READ_DIALECT = 'tsql'\n",
    "WRITE_DIALECT = 'postgres'\n",
    "\n",
    "DB_NAME = 'adventure_works_dw'\n",
    "SCHEMA = 'adventure_works'\n",
    "\n",
    "TRANSPILING_MANUAL = 'MANUAL'\n",
    "TRANSPILING_SQLGLOT = 'SQLGLOT'\n",
    "\n",
    "WRITE_DATA_PATH = '../data/database/postgres'\n",
    "TRANSPILED_SCRIPT_PATH = f'{WRITE_DATA_PATH}/create_adventure_works_postgres.sql'\n",
    "OUTPUT_ENCODING = 'UTF-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recodificar ficheros CSV de UTF-16 a UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a obtener todos los ficheros CSV almacenados en el directorio de lectura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/database/tsql/DatabaseLog.csv',\n",
       " '../data/database/tsql/DimAccount.csv',\n",
       " '../data/database/tsql/DimCurrency.csv',\n",
       " '../data/database/tsql/DimCustomer.csv',\n",
       " '../data/database/tsql/DimDate.csv',\n",
       " '../data/database/tsql/DimDepartmentGroup.csv',\n",
       " '../data/database/tsql/DimEmployee.csv',\n",
       " '../data/database/tsql/DimGeography.csv',\n",
       " '../data/database/tsql/DimOrganization.csv',\n",
       " '../data/database/tsql/DimProduct.csv',\n",
       " '../data/database/tsql/DimProductCategory.csv',\n",
       " '../data/database/tsql/DimProductSubcategory.csv',\n",
       " '../data/database/tsql/DimPromotion.csv',\n",
       " '../data/database/tsql/DimReseller.csv',\n",
       " '../data/database/tsql/DimSalesReason.csv',\n",
       " '../data/database/tsql/DimSalesTerritory.csv',\n",
       " '../data/database/tsql/DimScenario.csv',\n",
       " '../data/database/tsql/FactAdditionalInternationalProductDescription.csv',\n",
       " '../data/database/tsql/FactCallCenter.csv',\n",
       " '../data/database/tsql/FactCurrencyRate.csv',\n",
       " '../data/database/tsql/FactFinance.csv',\n",
       " '../data/database/tsql/FactInternetSales.csv',\n",
       " '../data/database/tsql/FactInternetSalesReason.csv',\n",
       " '../data/database/tsql/FactProductInventory.csv',\n",
       " '../data/database/tsql/FactResellerSales.csv',\n",
       " '../data/database/tsql/FactSalesQuota.csv',\n",
       " '../data/database/tsql/FactSurveyResponse.csv',\n",
       " '../data/database/tsql/NewFactCurrencyRate.csv',\n",
       " '../data/database/tsql/ProspectiveBuyer.csv',\n",
       " '../data/database/tsql/sysdiagrams.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files_list = []\n",
    "\n",
    "for file_path in Path(READ_DATA_PATH).rglob('*.csv'):\n",
    "    csv_files_list.append(file_path.as_posix())\n",
    "\n",
    "csv_files_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, para cada fichero en el listado, procederemos a abrirlo con el encoding de lectura, y a guardarlo con el encoding de escritura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DatabaseLog.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DatabaseLog.csv',\n",
       " 'DimAccount.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DimAccount.csv',\n",
       " 'DimCurrency.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DimCurrency.csv',\n",
       " 'DimCustomer.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DimCustomer.csv',\n",
       " 'DimDate.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DimDate.csv',\n",
       " 'DimDepartmentGroup.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DimDepartmentGroup.csv',\n",
       " 'DimEmployee.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DimEmployee.csv',\n",
       " 'DimGeography.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DimGeography.csv',\n",
       " 'DimOrganization.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DimOrganization.csv',\n",
       " 'DimProduct.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DimProduct.csv',\n",
       " 'DimProductCategory.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DimProductCategory.csv',\n",
       " 'DimProductSubcategory.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DimProductSubcategory.csv',\n",
       " 'DimPromotion.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DimPromotion.csv',\n",
       " 'DimReseller.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DimReseller.csv',\n",
       " 'DimSalesReason.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DimSalesReason.csv',\n",
       " 'DimSalesTerritory.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DimSalesTerritory.csv',\n",
       " 'DimScenario.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/DimScenario.csv',\n",
       " 'FactAdditionalInternationalProductDescription.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/FactAdditionalInternationalProductDescription.csv',\n",
       " 'FactCallCenter.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/FactCallCenter.csv',\n",
       " 'FactCurrencyRate.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/FactCurrencyRate.csv',\n",
       " 'FactFinance.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/FactFinance.csv',\n",
       " 'FactInternetSales.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/FactInternetSales.csv',\n",
       " 'FactInternetSalesReason.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/FactInternetSalesReason.csv',\n",
       " 'FactProductInventory.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/FactProductInventory.csv',\n",
       " 'FactResellerSales.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/FactResellerSales.csv',\n",
       " 'FactSalesQuota.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/FactSalesQuota.csv',\n",
       " 'FactSurveyResponse.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/FactSurveyResponse.csv',\n",
       " 'NewFactCurrencyRate.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/NewFactCurrencyRate.csv',\n",
       " 'ProspectiveBuyer.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/ProspectiveBuyer.csv',\n",
       " 'sysdiagrams.csv': 'C:/python/ucm-tfm-grupo-4/data/database/postgres/sysdiagrams.csv'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files_output_dict = {}\n",
    "\n",
    "for csv_file_path in csv_files_list:\n",
    "    csv_file_name = csv_file_path.split('/')[-1]\n",
    "    csv_output_path = f'{WRITE_DATA_PATH}/{csv_file_name}'\n",
    "\n",
    "    with open(csv_file_path, 'r', encoding=SAMPLES_ENCODING) as f:\n",
    "        csv_file = f.read()\n",
    "\n",
    "    with open(csv_output_path, 'w', encoding=OUTPUT_ENCODING) as f:\n",
    "        f.write(csv_file)\n",
    "\n",
    "    csv_files_output_dict[csv_file_name] = Path(csv_output_path).resolve().as_posix()\n",
    "\n",
    "csv_files_output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZjdXMtXv96k"
   },
   "source": [
    "# Traducción de Script T-SQL a Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kg3KK5zLSiaQ"
   },
   "source": [
    "Cargamos todo el script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1751485772460,
     "user": {
      "displayName": "Gastón Rimbano",
      "userId": "17074264266222486732"
     },
     "user_tz": -120
    },
    "id": "PLc0I0gtSIIf"
   },
   "outputs": [],
   "source": [
    "with open(FULL_SCRIPT_PATH, 'r', encoding= SQL_ENCODING) as f:\n",
    "    full_script = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJuCKplly0SR"
   },
   "source": [
    "Creamos funciones auxiliares y algunas definiciones adicionales a utilizar en el proceso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1751485772488,
     "user": {
      "displayName": "Gastón Rimbano",
      "userId": "17074264266222486732"
     },
     "user_tz": -120
    },
    "id": "FOLge73ry3Kx"
   },
   "outputs": [],
   "source": [
    "REPLACEMENT_DICTS = [\n",
    "    {'old': '$(DatabaseName)', 'new': DB_NAME},\n",
    "    {'old': '$(SqlSamplesSourceDataPath)', 'new': SAMPLES_ABS_PATH},\n",
    "    {'old': '[dbo]', 'new': f'[{SCHEMA}]'},\n",
    "    {'old': 'ON [PRIMARY]', 'new': ''},\n",
    "    {'old': 'WITH CHECK ADD', 'new': 'ADD'},\n",
    "    {'old': 'KEY NONCLUSTERED', 'new': 'KEY'},\n",
    "    {'old': 'KEY CLUSTERED', 'new': 'KEY'},\n",
    "    {'old': ' NONCLUSTERED INDEX', 'new': ' INDEX'},\n",
    "    {'old': ' CLUSTERED INDEX', 'new': ' INDEX'},\n",
    "]\n",
    "\n",
    "\n",
    "sections_params = [\n",
    "    {\n",
    "        'section': 'db',\n",
    "        'start_marker': '-- ****************************************\\n-- Create Database\\n-- ****************************************',\n",
    "        'end_marker': \"*** Checking for $(DatabaseName) Database';\\n\",\n",
    "        'transpiling': TRANSPILING_MANUAL\n",
    "    },\n",
    "    {\n",
    "        'section': 'tables',\n",
    "        'start_marker': '-- ******************************************************\\n-- Create tables\\n-- ******************************************************',\n",
    "        'end_marker': 'CREATE TABLE [dbo].[sysdiagrams](',\n",
    "        'ignore_sub_section': [\n",
    "            ('CREATE TABLE [dbo].[AdventureWorksDWBuildVersion](\\n', 'CREATE TABLE [dbo].[DimAccount](\\n')\n",
    "        ],\n",
    "        'transpiling': TRANSPILING_SQLGLOT\n",
    "    },\n",
    "    {\n",
    "        'section': 'load',\n",
    "        'start_marker': '-- ******************************************************\\n-- Load data\\n-- ******************************************************',\n",
    "        'end_marker': '-- ******************************************************\\n-- Add Primary Keys\\n-- ******************************************************',\n",
    "        'ignore_sub_section': [\n",
    "            (\"PRINT 'Loading [dbo].[AdventureWorksDWBuildVersion]';\\n\", \"PRINT 'Loading [dbo].[DimAccount]';\\n\")\n",
    "        ],\n",
    "        'transpiling': TRANSPILING_MANUAL\n",
    "    },\n",
    "    {\n",
    "        'section': 'primary_keys',\n",
    "        'start_marker': '-- ******************************************************\\n-- Add Primary Keys\\n-- ******************************************************',\n",
    "        'end_marker': '-- ******************************************************\\n-- Add Indexes\\n-- ******************************************************',\n",
    "        'ignore_sub_section': [\n",
    "            (\"PRINT '*** Adding Primary Keys';\\n\", \"ALTER TABLE [dbo].[DimAccount] WITH CHECK ADD\\n\")\n",
    "        ],\n",
    "        'transpiling': TRANSPILING_SQLGLOT\n",
    "    },\n",
    "    {\n",
    "        'section': 'indexes',\n",
    "        'start_marker': '-- ******************************************************\\n-- Add Indexes\\n-- ******************************************************',\n",
    "        'end_marker': '-- ****************************************\\n-- Create Foreign key constraints\\n-- ****************************************',\n",
    "        'transpiling': TRANSPILING_SQLGLOT\n",
    "    },\n",
    "    {\n",
    "        'section': 'foreign_key',\n",
    "        'start_marker': '-- ****************************************\\n-- Create Foreign key constraints\\n-- ****************************************',\n",
    "        'end_marker': '-- ******************************************************\\n-- Add database views.\\n-- ******************************************************',\n",
    "        'transpiling': TRANSPILING_SQLGLOT\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def trf_strip_upper(line: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes leading/trailing whitespace and converts a string to uppercase.\n",
    "\n",
    "    Args:\n",
    "        line: The input string.\n",
    "\n",
    "    Returns:\n",
    "        The stripped and uppercased string.\n",
    "    \"\"\"\n",
    "\n",
    "    return line.strip().upper()\n",
    "\n",
    "\n",
    "\n",
    "def replace_values(script: str, replacement_dicts: list = REPLACEMENT_DICTS) -> str:\n",
    "    \"\"\"\n",
    "    Replaces multiple values in a script string based on a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        script: The input script string.\n",
    "        replacement_dicts: A list of dictionaries, each with 'old' and 'new' keys\n",
    "                           for the replacement. Defaults to REPLACEMENT_DICTS.\n",
    "\n",
    "    Returns:\n",
    "        The script string with replaced values.\n",
    "    \"\"\"\n",
    "\n",
    "    for old_new_dict in replacement_dicts:\n",
    "        script = script.replace(old_new_dict['old'], old_new_dict['new'])\n",
    "\n",
    "    return script\n",
    "\n",
    "\n",
    "\n",
    "def clean_lines(script: str, section: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Cleans up a script string by removing specific lines (PRINT, GO, comments)\n",
    "    and applying section-specific cleaning rules.\n",
    "\n",
    "    Args:\n",
    "        script: The input script string.\n",
    "        section: The name of the script section (e.g., 'indexes') to apply\n",
    "                 section-specific rules. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        The cleaned script string.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in script.splitlines():\n",
    "        if trf_strip_upper(line).startswith('PRINT'):\n",
    "            continue\n",
    "\n",
    "        if trf_strip_upper(line).startswith('GO'):\n",
    "            continue\n",
    "\n",
    "        if trf_strip_upper(line).startswith('-- '):\n",
    "            continue\n",
    "\n",
    "        if section == 'indexes' and trf_strip_upper(line).startswith(')WITH'):\n",
    "            line = ');'\n",
    "\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "\n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "\n",
    "\n",
    "def manual_transpiling(script: str, section: str) -> str:\n",
    "    \"\"\"\n",
    "    Manually transpiles specific sections of the SQL script for Postgres.\n",
    "\n",
    "    Args:\n",
    "        script: The input script section string.\n",
    "        section: The name of the script section ('db' or 'load').\n",
    "\n",
    "    Returns:\n",
    "        The manually transpiled script section string.\n",
    "    \"\"\"\n",
    "\n",
    "    transpiled_lines = []\n",
    "\n",
    "    if section == 'db':\n",
    "        for line in script.strip().splitlines():\n",
    "            if not trf_strip_upper(line).startswith('CREATE DATABASE'):\n",
    "                transpiled_lines.append(line)\n",
    "                continue\n",
    "            \n",
    "            transpiled_lines.append(f'\\nDROP DATABASE IF EXISTS {DB_NAME};\\n\\n')\n",
    "            transpiled_lines.append(line)\n",
    "            transpiled_lines.append(f'\\n\\\\c {DB_NAME};')\n",
    "            transpiled_lines.append(f'\\nCREATE SCHEMA {SCHEMA};')\n",
    "            transpiled_lines.append('\\nDROP SCHEMA IF EXISTS public CASCADE;')\n",
    "\n",
    "\n",
    "    if section == 'load':\n",
    "        for line in script.splitlines():\n",
    "            if not trf_strip_upper(line).startswith('BULK INSERT'):\n",
    "                continue\n",
    "\n",
    "            line = line.replace('BULK INSERT', 'COPY')\n",
    "            line = line.replace('[', '\"')\n",
    "            line = line.replace(']', '\"')\n",
    "\n",
    "            csv_file_name = line.split(\"'\")[1]\n",
    "            line = line.replace(\n",
    "                csv_file_name,\n",
    "                csv_files_output_dict[csv_file_name]\n",
    "            )\n",
    "            \n",
    "            line += ' WITH ('\n",
    "            transpiled_lines.append(line)\n",
    "\n",
    "            transpiled_lines.append(f\"    DELIMITER '{SAMPLES_DELIMITER}',\")\n",
    "            transpiled_lines.append(\"    FORMAT CSV,\")\n",
    "            transpiled_lines.append(\"    HEADER FALSE,\")\n",
    "            transpiled_lines.append(f\"    ENCODING '{OUTPUT_ENCODING.replace('-', '')}'\")\n",
    "            transpiled_lines.append(\");\\n\")\n",
    "\n",
    "    return '\\n'.join(transpiled_lines)\n",
    "\n",
    "\n",
    "\n",
    "def final_post_processing(script: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs final post-processing on the transpiled script, including data type\n",
    "    replacements and removal of T-SQL specific keywords.\n",
    "\n",
    "    Args:\n",
    "        script: The transpiled script string.\n",
    "\n",
    "    Returns:\n",
    "        The final post-processed script string.\n",
    "    \"\"\"\n",
    "\n",
    "    script = script.replace(' GENERATED AS ', ' GENERATED BY DEFAULT AS ')\n",
    "    script = script.replace(' bit ', ' BOOLEAN ').replace(' BIT ', ' BOOLEAN ')\n",
    "    script = script.replace(' money ', ' NUMERIC(19,4) ').replace(' MONEY ', ' NUMERIC(19,4) ')\n",
    "    script = script.replace(' BYTEA(MAX) ', ' BYTEA ')\n",
    "    script = script.replace(' VARCHAR(MAX) ', ' TEXT ')\n",
    "    script = script.replace(' NONCLUSTERED', '').replace(' CLUSTERED', '')\n",
    "    script = script.replace(')\\n\\nCREATE TABLE', ');\\n\\nCREATE TABLE')\n",
    "\n",
    "    return script\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjlXO45USlgP"
   },
   "source": [
    "A continuación, implementamos el script que permitirá:\n",
    "\n",
    "1.   Capturar las secciones de interés del script, excluyendo de ella los fragmentos no deseados.\n",
    "2.   Aplicar transofrmaciones de limpieza sobre la estructura de las secciones de interés.\n",
    "3.  Traducir del dialecto de entrada (T-SQL) al de salida (Postgres) las secciones deseadas.\n",
    "4.  Finalmente, unificará todas secciones traducidas, y las exportará a un fichero `.sql`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1751485772722,
     "user": {
      "displayName": "Gastón Rimbano",
      "userId": "17074264266222486732"
     },
     "user_tz": -120
    },
    "id": "pDXCNoB_VCl6",
    "outputId": "354d5188-b35c-4dff-a9cd-df57ff6875c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transpiling section `db` ...\n",
      "> Start index: 2603\n",
      "> End index: 2809\n",
      "> tsql section obtained\n",
      "> tsql section cleaned\n",
      "> Section transpiled (MANUAL) tsql --> postgres\n",
      "\n",
      "Transpiling section `tables` ...\n",
      "> Start index: 7004\n",
      "> End index: 21633\n",
      "> tsql section obtained\n",
      "> Sub-sections ignored\n",
      "> tsql section cleaned\n",
      "> Section transpiled (SQLGLOT) tsql --> postgres\n",
      "\n",
      "Transpiling section `load` ...\n",
      "> Start index: 22079\n",
      "> End index: 31133\n",
      "> tsql section obtained\n",
      "> Sub-sections ignored\n",
      "> tsql section cleaned\n",
      "> Section transpiled (MANUAL) tsql --> postgres\n",
      "\n",
      "Transpiling section `primary_keys` ...\n",
      "> Start index: 31133\n",
      "> End index: 36504\n",
      "> tsql section obtained\n",
      "> Sub-sections ignored\n",
      "> tsql section cleaned\n",
      "> Section transpiled (SQLGLOT) tsql --> postgres\n",
      "\n",
      "Transpiling section `indexes` ...\n",
      "> Start index: 36504\n",
      "> End index: 40192\n",
      "> tsql section obtained\n",
      "> tsql section cleaned\n",
      "> Section transpiled (SQLGLOT) tsql --> postgres\n",
      "\n",
      "Transpiling section `foreign_key` ...\n",
      "> Start index: 40192\n",
      "> End index: 47319\n",
      "> tsql section obtained\n",
      "> tsql section cleaned\n",
      "> Section transpiled (SQLGLOT) tsql --> postgres\n",
      "\n",
      "--> Transpiled script saved in `../data/database/postgres/create_adventure_works_postgres.sql` <--\n"
     ]
    }
   ],
   "source": [
    "output_transpiled_script = ''\n",
    "\n",
    "for section_data in sections_params:\n",
    "    section = section_data['section']\n",
    "    print(f'Transpiling section `{section}` ...')\n",
    "\n",
    "    start_marker = section_data['start_marker']\n",
    "    end_marker = section_data['end_marker']\n",
    "\n",
    "    start_index = full_script.find(start_marker)\n",
    "    end_index = full_script.find(end_marker)\n",
    "\n",
    "    print(f'> Start index: {start_index}')\n",
    "    print(f'> End index: {end_index}')\n",
    "\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        section_script = full_script[start_index:end_index]\n",
    "        section_data[f'{READ_DIALECT}_script'] = section_script\n",
    "        print(f'> {READ_DIALECT} section obtained')\n",
    "\n",
    "\n",
    "    cleaned_script = section_script\n",
    "    if section_data.get('ignore_sub_section'):\n",
    "        init_index = 0\n",
    "        cleaned_script = ''\n",
    "\n",
    "        for ignore_start, ignore_end in section_data['ignore_sub_section']:\n",
    "            ignore_start_index = section_script.find(ignore_start)\n",
    "            cleaned_script += section_script[init_index:ignore_start_index]\n",
    "\n",
    "            init_index = section_script.find(ignore_end)\n",
    "\n",
    "        cleaned_script += section_script[init_index:]\n",
    "        print(f'> Sub-sections ignored')\n",
    "\n",
    "    cleaned_script = replace_values(cleaned_script)\n",
    "    cleaned_script = clean_lines(cleaned_script, section)\n",
    "    section_data['cleaned_script'] = cleaned_script\n",
    "    print(f'> {READ_DIALECT} section cleaned')\n",
    "\n",
    "\n",
    "    if section_data['transpiling'] == TRANSPILING_SQLGLOT:\n",
    "        transpiled_script = sqlglot.transpile(\n",
    "            cleaned_script,\n",
    "            read= READ_DIALECT,\n",
    "            write= WRITE_DIALECT,\n",
    "            pretty= True,\n",
    "            indent= 4,\n",
    "            pad= 4,\n",
    "            normalize_functions= 'upper'\n",
    "        )\n",
    "        section_data[f'{WRITE_DIALECT}_script'] = '\\n\\n'.join(transpiled_script)\n",
    "\n",
    "    if section_data['transpiling'] == TRANSPILING_MANUAL:\n",
    "        section_data[f'{WRITE_DIALECT}_script'] = manual_transpiling(cleaned_script, section)\n",
    "\n",
    "    print(f\"> Section transpiled ({section_data['transpiling']}) {READ_DIALECT} --> {WRITE_DIALECT}\")\n",
    "\n",
    "    output_transpiled_script += section_data['start_marker']\n",
    "    output_transpiled_script += '\\n' * 2\n",
    "    output_transpiled_script += final_post_processing(section_data[f'{WRITE_DIALECT}_script'])\n",
    "    output_transpiled_script += '\\n' * 4\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "with open(TRANSPILED_SCRIPT_PATH, 'w', encoding= OUTPUT_ENCODING) as f:\n",
    "    f.write(output_transpiled_script)\n",
    "\n",
    "print(f'--> Transpiled script saved in `{TRANSPILED_SCRIPT_PATH}` <--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMwjgxVKEaksp61FrXH8xnm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ucm-tfm-grupo-4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
