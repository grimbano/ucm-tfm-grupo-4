{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traducción de Script de Creación de la BBDD de T-SQL a Postgres\n",
    "\n",
    "En el presente Notebook se ejecutarán todos los comandos necesarios para poder traducir el script de creación de la BBDD de \"dialecto\" T-SQL a Postgres. Para esto, se deben codificar los CSV con datos en un formato admisible para Postgres (UTF-8) y traducir el script en si. Para esto último, nos valdremos del uso de un LLM de Azure Open AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QPTZC3Bsv0LG"
   },
   "source": [
    "## Librerías\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 531,
     "status": "ok",
     "timestamp": 1751485772446,
     "user": {
      "displayName": "Gastón Rimbano",
      "userId": "17074264266222486732"
     },
     "user_tz": -120
    },
    "id": "MlTiSQvkv0jZ"
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5pAKrj5IIPz"
   },
   "source": [
    "## Variables globales\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1751485772454,
     "user": {
      "displayName": "Gastón Rimbano",
      "userId": "17074264266222486732"
     },
     "user_tz": -120
    },
    "id": "stToljf2_p99"
   },
   "outputs": [],
   "source": [
    "load_dotenv('../.env')\n",
    "\n",
    "READ_DATA_PATH = '../data/database/tsql/db_creation'\n",
    "FULL_SCRIPT_PATH = f'{READ_DATA_PATH}/instawdbdw.sql'\n",
    "SQL_ENCODING = 'UTF-16'\n",
    "\n",
    "SAMPLES_ENCODING = 'UTF16'\n",
    "SAMPLES_DELIMITER = '|'\n",
    "\n",
    "READ_DIALECT = 'tsql'\n",
    "WRITE_DIALECT = 'postgres'\n",
    "\n",
    "DB_NAME = 'adventure_works_dw'\n",
    "SCHEMA = 'adventure_works'\n",
    "\n",
    "TRANSPILING_MANUAL = 'MANUAL'\n",
    "TRANSPILING_SQLGLOT = 'SQLGLOT'\n",
    "TRANSPILING_LLM = 'LLM'\n",
    "\n",
    "WRITE_DATA_PATH = '../data/database/postgres/db_creation'\n",
    "TRANSPILED_SCRIPT_PATH = f'{WRITE_DATA_PATH}/create_adventure_works_postgres.sql'\n",
    "OUTPUT_ENCODING = 'UTF-8'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recodificar ficheros CSV de UTF-16 a UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procedemos a obtener todos los ficheros CSV almacenados en el directorio de lectura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/database/tsql/db_creation/DimAccount.csv',\n",
       " '../data/database/tsql/db_creation/DimCurrency.csv',\n",
       " '../data/database/tsql/db_creation/DimCustomer.csv',\n",
       " '../data/database/tsql/db_creation/DimDate.csv',\n",
       " '../data/database/tsql/db_creation/DimDepartmentGroup.csv',\n",
       " '../data/database/tsql/db_creation/DimEmployee.csv',\n",
       " '../data/database/tsql/db_creation/DimGeography.csv',\n",
       " '../data/database/tsql/db_creation/DimOrganization.csv',\n",
       " '../data/database/tsql/db_creation/DimProduct.csv',\n",
       " '../data/database/tsql/db_creation/DimProductCategory.csv',\n",
       " '../data/database/tsql/db_creation/DimProductSubcategory.csv',\n",
       " '../data/database/tsql/db_creation/DimPromotion.csv',\n",
       " '../data/database/tsql/db_creation/DimReseller.csv',\n",
       " '../data/database/tsql/db_creation/DimSalesReason.csv',\n",
       " '../data/database/tsql/db_creation/DimSalesTerritory.csv',\n",
       " '../data/database/tsql/db_creation/DimScenario.csv',\n",
       " '../data/database/tsql/db_creation/FactAdditionalInternationalProductDescription.csv',\n",
       " '../data/database/tsql/db_creation/FactCallCenter.csv',\n",
       " '../data/database/tsql/db_creation/FactCurrencyRate.csv',\n",
       " '../data/database/tsql/db_creation/FactFinance.csv',\n",
       " '../data/database/tsql/db_creation/FactInternetSales.csv',\n",
       " '../data/database/tsql/db_creation/FactInternetSalesReason.csv',\n",
       " '../data/database/tsql/db_creation/FactProductInventory.csv',\n",
       " '../data/database/tsql/db_creation/FactResellerSales.csv',\n",
       " '../data/database/tsql/db_creation/FactSalesQuota.csv',\n",
       " '../data/database/tsql/db_creation/FactSurveyResponse.csv',\n",
       " '../data/database/tsql/db_creation/NewFactCurrencyRate.csv',\n",
       " '../data/database/tsql/db_creation/ProspectiveBuyer.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excluding_csv_list = [\n",
    "    'DatabaseLog.csv',\n",
    "    'sysdiagrams.csv',\n",
    "]\n",
    "csv_files_list = []\n",
    "\n",
    "for file_path in Path(READ_DATA_PATH).rglob('*.csv'):\n",
    "    exclude_csv = [file_path.as_posix().endswith(exclude_csv_name) for exclude_csv_name in excluding_csv_list]\n",
    "\n",
    "    if any(exclude_csv):\n",
    "        continue\n",
    "\n",
    "    csv_files_list.append(file_path.as_posix())\n",
    "\n",
    "csv_files_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, para cada fichero en el listado, procederemos a abrirlo con el encoding de lectura, y a guardarlo con el encoding de escritura:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/python/ucm-tfm-grupo-4/data/database/postgres/db_creation/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLES_ABS_PATH = Path(WRITE_DATA_PATH).resolve().as_posix() + '/'\n",
    "SAMPLES_ABS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for csv_file_path in csv_files_list:\n",
    "    csv_file_name = csv_file_path.split('/')[-1]\n",
    "    csv_output_path = f'{SAMPLES_ABS_PATH}{csv_file_name}'\n",
    "\n",
    "    with open(csv_file_path, 'r', encoding=SAMPLES_ENCODING) as r_f, \\\n",
    "        open(csv_output_path, 'w', encoding=OUTPUT_ENCODING, newline='') as w_f:\n",
    "\n",
    "        reader = csv.reader(r_f, delimiter=SAMPLES_DELIMITER, quotechar='\"', doublequote=True)\n",
    "        writer = csv.writer(w_f, delimiter=SAMPLES_DELIMITER, quotechar='\"', doublequote=True, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        for row in reader:\n",
    "            writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xZjdXMtXv96k"
   },
   "source": [
    "## Traducción de Script T-SQL a Postgres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kg3KK5zLSiaQ"
   },
   "source": [
    "Cargamos todo el script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1751485772460,
     "user": {
      "displayName": "Gastón Rimbano",
      "userId": "17074264266222486732"
     },
     "user_tz": -120
    },
    "id": "PLc0I0gtSIIf"
   },
   "outputs": [],
   "source": [
    "with open(FULL_SCRIPT_PATH, 'r', encoding= SQL_ENCODING) as f:\n",
    "    full_script = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UJuCKplly0SR"
   },
   "source": [
    "Creamos funciones auxiliares y algunas definiciones adicionales a utilizar en el proceso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1751485772488,
     "user": {
      "displayName": "Gastón Rimbano",
      "userId": "17074264266222486732"
     },
     "user_tz": -120
    },
    "id": "FOLge73ry3Kx"
   },
   "outputs": [],
   "source": [
    "REPLACEMENT_DICTS = [\n",
    "    {'old': '$(DatabaseName)', 'new': DB_NAME},\n",
    "    {'old': '$(SqlSamplesSourceDataPath)', 'new': SAMPLES_ABS_PATH},\n",
    "    {'old': '[dbo]', 'new': f'[{SCHEMA}]'},\n",
    "]\n",
    "\n",
    "\n",
    "sections_params = [\n",
    "    {\n",
    "        'section': 'db',\n",
    "        'start_marker': '-- ****************************************\\n-- Create Database\\n-- ****************************************',\n",
    "        'end_marker': \"*** Checking for $(DatabaseName) Database';\\n\",\n",
    "    },\n",
    "    {\n",
    "        'section': 'tables',\n",
    "        'start_marker': '-- ******************************************************\\n-- Create tables\\n-- ******************************************************',\n",
    "        'end_marker': 'CREATE TABLE [dbo].[sysdiagrams](',\n",
    "        'ignore_sub_section': [\n",
    "            ('CREATE TABLE [dbo].[AdventureWorksDWBuildVersion](\\n', 'CREATE TABLE [dbo].[DimAccount](\\n')\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        'section': 'load',\n",
    "        'start_marker': '-- ******************************************************\\n-- Load data\\n-- ******************************************************',\n",
    "        'end_marker': '-- ******************************************************\\n-- Add Primary Keys\\n-- ******************************************************',\n",
    "        'ignore_sub_section': [\n",
    "            (\"PRINT 'Loading [dbo].[AdventureWorksDWBuildVersion]';\\n\", \"PRINT 'Loading [dbo].[DimAccount]';\\n\"),\n",
    "            (\"PRINT 'Loading [dbo].[sysdiagrams]';\", \"-- ******************************************************\\n-- Add Primary Keys\\n-- ******************************************************\")\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        'section': 'primary_keys',\n",
    "        'start_marker': '-- ******************************************************\\n-- Add Primary Keys\\n-- ******************************************************',\n",
    "        'end_marker': '-- ******************************************************\\n-- Add Indexes\\n-- ******************************************************',\n",
    "        'ignore_sub_section': [\n",
    "            (\"PRINT '*** Adding Primary Keys';\\n\", \"ALTER TABLE [dbo].[DimAccount] WITH CHECK ADD\\n\")\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        'section': 'indexes',\n",
    "        'start_marker': '-- ******************************************************\\n-- Add Indexes\\n-- ******************************************************',\n",
    "        'end_marker': '-- ****************************************\\n-- Create Foreign key constraints\\n-- ****************************************',\n",
    "        'ignore_sub_section': [\n",
    "            ('CREATE UNIQUE NONCLUSTERED INDEX [UK_principal_name] ON [dbo].[sysdiagrams]([principal_id],\\t[name]) ON [PRIMARY];', '-- ****************************************\\n-- Create Foreign key constraints\\n-- ****************************************')\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        'section': 'foreign_key',\n",
    "        'start_marker': '-- ****************************************\\n-- Create Foreign key constraints\\n-- ****************************************',\n",
    "        'end_marker': '-- ******************************************************\\n-- Add database views.\\n-- ******************************************************',\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def trf_strip_upper(line: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes leading/trailing whitespace and converts a string to uppercase.\n",
    "\n",
    "    Args:\n",
    "        line: The input string.\n",
    "\n",
    "    Returns:\n",
    "        The stripped and uppercased string.\n",
    "    \"\"\"\n",
    "\n",
    "    return line.strip().upper()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def replace_values(script: str, replacement_dicts: list = REPLACEMENT_DICTS) -> str:\n",
    "    \"\"\"\n",
    "    Replaces multiple values in a script string based on a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        script: The input script string.\n",
    "        replacement_dicts: A list of dictionaries, each with 'old' and 'new' keys\n",
    "                           for the replacement. Defaults to REPLACEMENT_DICTS.\n",
    "\n",
    "    Returns:\n",
    "        The script string with replaced values.\n",
    "    \"\"\"\n",
    "\n",
    "    for old_new_dict in replacement_dicts:\n",
    "        script = script.replace(old_new_dict['old'], old_new_dict['new'])\n",
    "\n",
    "    return script\n",
    "\n",
    "\n",
    "\n",
    "def clean_lines(script: str, section: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Cleans up a script string by removing specific lines (PRINT, GO, comments)\n",
    "    and applying section-specific cleaning rules.\n",
    "\n",
    "    Args:\n",
    "        script: The input script string.\n",
    "        section: The name of the script section (e.g., 'indexes') to apply\n",
    "                 section-specific rules. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        The cleaned script string.\n",
    "    \"\"\"\n",
    "\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in script.splitlines():\n",
    "        if trf_strip_upper(line).startswith('PRINT'):\n",
    "            continue\n",
    "\n",
    "        if trf_strip_upper(line).startswith('-- '):\n",
    "            continue\n",
    "\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "\n",
    "    return '\\n'.join(cleaned_lines)\n",
    "\n",
    "\n",
    "\n",
    "def llm_transpiling(script: str, input_dialect: str = READ_DIALECT, \n",
    "                    output_dialect:str = WRITE_DIALECT) -> str:\n",
    "    \"\"\"\n",
    "    Uses an LLM to transpile specific sections of the SQL `script` from\n",
    "    `input_dialect` to `output_dialect`.\n",
    "\n",
    "    Args:\n",
    "        script: The input script section string.\n",
    "        input_dialect: The input SQL dialect.\n",
    "        output_dialect: The output SQL dialect.\n",
    "\n",
    "    Returns:\n",
    "        The transpiled script section string.\n",
    "    \"\"\"\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "    # CONTEXT #\n",
    "    You are a database administrator and expert in SQL dialects. Your role is to **transpile DDL commands** from `{input_dialect}` to `{output_dialect}`. \\\n",
    "    The user will provide the contents of an `.sql` file containing `{input_dialect}` DDL commands.\n",
    "\n",
    "    # OBJECTIVE #\n",
    "    Your task is to output the **transpiled `{output_dialect}`-compatible DDL commands**, ready for execution.\n",
    "\n",
    "    # INSTRUCTIONS #\n",
    "    Your response must be a **plain text output** of valid DDL commands **for `{output_dialect}` only**, following **these strict rules**:\n",
    "\n",
    "    - ❌ Do not use markdown tags (e.g., ` ```sql ` or similar).\n",
    "    - ✅ Convert all entity names (tables, columns, schemas) from camelCase or PascalCase to `snake_case`.\n",
    "\n",
    "    - ✅ Ensure the following database setup:\n",
    "        - If the database `{DB_NAME}` already exists, drop it and recreate it from scratch. Then, connect to it.\n",
    "        - Create schema `{SCHEMA}` within `{DB_NAME}`.\n",
    "        - Drop schema `public` from `{DB_NAME}` if it exists.\n",
    "\n",
    "    - ✅ Process table creation:\n",
    "        - Create all tables under schema `{SCHEMA}`.\n",
    "        - Do **not** define any `PRIMARY KEY`, `FOREIGN KEY`, or `INDEX` inside the `CREATE TABLE` statements.\n",
    "        - Ignore `NOT NULL` constraints **only** for columns named `..._product_name` in the `dim_product` table.\n",
    "\n",
    "    - ✅ Include data loading steps:\n",
    "        - Use `COPY` from CSV for each table with this clause:  \n",
    "            `WITH (DELIMITER '{SAMPLES_DELIMITER}', FORMAT CSV, HEADER FALSE, ENCODING '{OUTPUT_ENCODING.replace('-', '')}')`\n",
    "        - ⚠️ **Do not modify the provided file path** for the CSV files under any circumstances.\n",
    "\n",
    "    - ✅ After loading data with `COPY`, apply the following **in strict order**:\n",
    "        1. Add `PRIMARY KEY` constraints using `ALTER TABLE` statements.\n",
    "        2. Add `INDEXES` for relevant columns (e.g., those used in joins or foreign keys).\n",
    "        3. Add `FOREIGN KEY` constraints using `ALTER TABLE ... ADD CONSTRAINT ... FOREIGN KEY ... REFERENCES ...`.\n",
    "\n",
    "    - ⚠️ Ensure all constraint and index names are valid and do not exceed `{output_dialect}` maximum identifier length.\n",
    "    - ✅ You may generate constraint and index names automatically using a consistent naming pattern based on table and column names.\n",
    "\n",
    "    - ✅ If a data type, default value or constraint is not supported in `{output_dialect}`, choose the **closest compatible alternative** silently, without warnings or explanations.\n",
    "\n",
    "    # RESPONSE FORMAT #\n",
    "    Only output valid `{output_dialect}` DDL commands. Do not include comments or explanations unless they are part of the original SQL or required by the syntax.\n",
    "    \"\"\"\n",
    "\n",
    "    client = AzureOpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model= 'gpt-5-mini',\n",
    "        # temperature= 0.0,\n",
    "        messages= [\n",
    "            {'role': 'system', 'content': system_prompt},\n",
    "            {'role': 'user', 'content': script}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjlXO45USlgP"
   },
   "source": [
    "A continuación, implementamos el script que permitirá:\n",
    "\n",
    "1.   Capturar las secciones de interés del script, excluyendo de ella los fragmentos no deseados.\n",
    "2.   Aplicar transofrmaciones de limpieza sobre la estructura de las secciones de interés.\n",
    "3.  Traducir del dialecto de entrada (T-SQL) al de salida (Postgres) las secciones deseadas.\n",
    "4.  Finalmente, unificará todas secciones traducidas, y las exportará a un fichero `.sql`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1751485772722,
     "user": {
      "displayName": "Gastón Rimbano",
      "userId": "17074264266222486732"
     },
     "user_tz": -120
    },
    "id": "pDXCNoB_VCl6",
    "outputId": "354d5188-b35c-4dff-a9cd-df57ff6875c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting reduced section `db` ...\n",
      "> Start index: 2603\n",
      "> End index: 2809\n",
      "> tsql section obtained\n",
      "> tsql section cleaned\n",
      "\n",
      "Getting reduced section `tables` ...\n",
      "> Start index: 7004\n",
      "> End index: 21633\n",
      "> tsql section obtained\n",
      "> Sub-sections ignored\n",
      "> tsql section cleaned\n",
      "\n",
      "Getting reduced section `load` ...\n",
      "> Start index: 22079\n",
      "> End index: 31133\n",
      "> tsql section obtained\n",
      "> Sub-sections ignored\n",
      "> tsql section cleaned\n",
      "\n",
      "Getting reduced section `primary_keys` ...\n",
      "> Start index: 31133\n",
      "> End index: 36504\n",
      "> tsql section obtained\n",
      "> Sub-sections ignored\n",
      "> tsql section cleaned\n",
      "\n",
      "Getting reduced section `indexes` ...\n",
      "> Start index: 36504\n",
      "> End index: 40192\n",
      "> tsql section obtained\n",
      "> Sub-sections ignored\n",
      "> tsql section cleaned\n",
      "\n",
      "Getting reduced section `foreign_key` ...\n",
      "> Start index: 40192\n",
      "> End index: 47319\n",
      "> tsql section obtained\n",
      "> tsql section cleaned\n",
      "\n",
      ">> Reduced script obtained.\n",
      ">> LLM's transpiling ...\n",
      "\n",
      "--> Transpiled script saved in `../data/database/postgres/db_creation/create_adventure_works_postgres.sql` <--\n"
     ]
    }
   ],
   "source": [
    "reduced_script = ''\n",
    "\n",
    "for section_data in sections_params:\n",
    "    section = section_data['section']\n",
    "    print(f'Getting reduced section `{section}` ...')\n",
    "\n",
    "    start_marker = section_data['start_marker']\n",
    "    end_marker = section_data['end_marker']\n",
    "\n",
    "    start_index = full_script.find(start_marker)\n",
    "    end_index = full_script.find(end_marker)\n",
    "\n",
    "    print(f'> Start index: {start_index}')\n",
    "    print(f'> End index: {end_index}')\n",
    "\n",
    "    if start_index != -1 and end_index != -1:\n",
    "        section_script = full_script[start_index:end_index]\n",
    "        section_data[f'{READ_DIALECT}_script'] = section_script\n",
    "        print(f'> {READ_DIALECT} section obtained')\n",
    "\n",
    "\n",
    "    cleaned_script = section_script\n",
    "    if section_data.get('ignore_sub_section'):\n",
    "        init_index = 0\n",
    "        cleaned_script = ''\n",
    "\n",
    "        for ignore_start, ignore_end in section_data['ignore_sub_section']:\n",
    "            ignore_start_index = section_script.find(ignore_start)\n",
    "            cleaned_script += section_script[init_index:ignore_start_index]\n",
    "\n",
    "            init_index = section_script.find(ignore_end)\n",
    "\n",
    "        cleaned_script += section_script[init_index:]\n",
    "        print(f'> Sub-sections ignored')\n",
    "\n",
    "    cleaned_script = replace_values(cleaned_script)\n",
    "    cleaned_script = clean_lines(cleaned_script, section)\n",
    "    section_data['cleaned_script'] = cleaned_script\n",
    "    print(f'> {READ_DIALECT} section cleaned')\n",
    "\n",
    "    reduced_script += section_data['start_marker']\n",
    "    reduced_script += '\\n' * 2\n",
    "    reduced_script += section_data['cleaned_script']\n",
    "    reduced_script += '\\n' * 4\n",
    "\n",
    "    print()\n",
    "\n",
    "print('>> Reduced script obtained.')\n",
    "print(\">> LLM's transpiling ...\")\n",
    "\n",
    "with open(TRANSPILED_SCRIPT_PATH, 'w', encoding= OUTPUT_ENCODING) as f:\n",
    "    f.write(llm_transpiling(reduced_script))\n",
    "\n",
    "print()\n",
    "print(f'--> Transpiled script saved in `{TRANSPILED_SCRIPT_PATH}` <--')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMwjgxVKEaksp61FrXH8xnm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ucm-tfm-grupo-4 (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
